"""
Chatbot service using Gemini AI and Neo4j Knowledge Graph
Optimized for speed with streaming, caching, and reduced API calls
"""
from __future__ import annotations
import asyncio
import json
import time
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime, timedelta

import google.generativeai as genai
from config import GOOGLE_API_KEY, GEMINI_MODEL, NEO4J_DATABASE, CACHE_TTL
from services.neo4j_exec import connect_neo4j, execute_cypher
from services.query_loader import load_cypher_queries

# Initialize Gemini
genai.configure(api_key=GOOGLE_API_KEY)

# Load Cypher Query Templates from file
print("Loading Cypher queries from file...")
QUERY_TEMPLATES = load_cypher_queries()
print(f"Loaded {len(QUERY_TEMPLATES)} query templates")

# Simple in-memory cache for responses
_response_cache: Dict[str, tuple[Any, datetime]] = {}


def _get_cache(key: str) -> Optional[Any]:
    """Get cached response if not expired"""
    key = key.lower().strip()
    if key in _response_cache:
        value, timestamp = _response_cache[key]
        if datetime.now() - timestamp < timedelta(seconds=CACHE_TTL):
            return value
        else:
            del _response_cache[key]
    return None


def _set_cache(key: str, value: Any) -> None:
    """Set cache with current timestamp"""
    key = key.lower().strip()
    _response_cache[key] = (value, datetime.now())


async def detect_intent(user_query: str, system_prompt: str) -> Dict[str, Any]:
    """
    Detect user intent and extract entities using Gemini
    """
    # Quick check for greetings to save API calls
    greetings = {"hi", "hello", "xin chÃ o", "chÃ o", "chao", "hola"}
    if user_query.lower().strip() in greetings:
        return {
            "intent": "GREETING",
            "entities": {},
            "query_type": "greeting"
        }

    # Force re-configure to ensure key is active in this thread/process
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel(model_name=GEMINI_MODEL)
    
    # --- Token Counting (Estimate) ---
    try:
        count = await model.count_tokens_async(prompt)
        print(f"ðŸ“Š Intent Tokens: {count.total_tokens}")
    except:
        pass
    # ---------------------------------
    
    # Optimized prompt - shorter and more direct
    prompt = f"""
    PhÃ¢n tÃ­ch cÃ¢u há»i sau Ä‘á»ƒ láº¥y thÃ´ng tin truy váº¥n graph database:
    User: "{user_query}"
    
    Tráº£ vá» JSON:
    {{
        "intent": "STUDY|VISA|SETTLEMENT|PATHWAY|COMPARE",
        "entities": {{
             // TrÃ­ch xuáº¥t keyword quan trá»ng: university_name, level, field, exam_type, score, visa_subclass...
        }},
        "query_type": "map_to_predefined_query_name"
    }}
    """
    
    try:
        # Use native async method
        response = await model.generate_content_async(prompt)
        text = response.text.strip()
        
        # Clean up json markdown code blocks if present
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
        return json.loads(text.strip())
    except Exception as e:
        error_str = str(e)
        print(f"âŒ GEMINI ERROR DETAILS: {error_str}")
        if "429" in error_str or "quota" in error_str.lower() or "ResourceExhausted" in error_str:
            print("âš ï¸ Google API Quota Exceeded (Confirmed)")
            return {
                "intent": "QUOTA_ERROR",
                "entities": {},
                "query_type": "quota_error"
            }
        print(f"Intent detection error: {e}")
        return {
            "intent": "STUDY",
            "entities": {},
            "query_type": "fallback"
        }


async def execute_query(query_type: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Execute Cypher query against Neo4j (Async wrapper)
    """
    if query_type not in QUERY_TEMPLATES:
        return []
    
    query = QUERY_TEMPLATES[query_type]
    
    def _run_query():
        driver = connect_neo4j()
        if not driver:
            return []
        try:
            with driver.session(database=NEO4J_DATABASE) as session:
                result = session.run(query, **params)
                data = [record.data() for record in result]
                return data
        except Exception as e:
            print(f"Query execution error: {e}")
            return []
        finally:
            driver.close()

    return await asyncio.to_thread(_run_query)


async def format_response(user_query: str, query_results: List[Dict[str, Any]], system_prompt: str) -> str:
    """
    Format query results into natural language response using Gemini (Async)
    """
    model = genai.GenerativeModel(model_name=GEMINI_MODEL)
    
    prompt = f"""
    {system_prompt}
    User: "{user_query}"
    Data: {json.dumps(query_results[:5], ensure_ascii=False)} # Limit context size
    
    HÃ£y tráº£ lá»i tháº­t sinh Ä‘á»™ng vÃ  báº¯t máº¯t:
    1. Báº®T BUá»˜C dÃ¹ng biá»ƒu tÆ°á»£ng (emoji) cho Táº¤T Cáº¢ cÃ¡c tiÃªu Ä‘á» vÃ  Ã½ chÃ­nh.
    2. VÃ­ dá»¥: ðŸŽ“ Du há»c, ðŸ›‚ Visa, ðŸ’° Chi phÃ­, ðŸ“… Thá»i gian, âœ… Äiá»u kiá»‡n, ðŸ« TrÆ°á»ng há»c.
    3. TrÃ¬nh bÃ y dáº¡ng danh sÃ¡ch (bullet points) dá»… Ä‘á»c.
    """
    
    try:
        # Count tokens before generating
        token_count = await model.count_tokens_async(prompt)
        print(f"\nðŸ“Š TOKEN USAGE ESTIMATE:")
        print(f"   - Input Tokens: {token_count.total_tokens}")
        print(f"   - Est. Cost (Free Tier): 0$")
        print(f"   - Remaining Requests (Daily Limit ~1500): Check Google Console\n")

        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        print(f"Response formatting error: {e}")
        return "Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u tráº£ lá»i."


async def chatbot_response(user_query: str, system_prompt: str) -> Dict[str, Any]:
    """
    Main chatbot function - Async
    """
    # Step 1: Detect intent
    analysis = await detect_intent(user_query, system_prompt)
    
    if analysis.get("query_type") == "greeting":
        return {
            "response": "ChÃ o báº¡n! TÃ´i lÃ  trá»£ lÃ½ áº£o AusVisa. TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n vá» du há»c vÃ  visa Ãšc?",
            "intent": "GREETING",
            "query_results": []
        }

    if analysis.get("intent") == "QUOTA_ERROR":
         return {
            "response": "âš ï¸ Há»‡ thá»‘ng Ä‘ang quÃ¡ táº£i (Google API Quota Exceeded). Vui lÃ²ng thá»­ láº¡i sau vÃ i phÃºt hoáº·c liÃªn há»‡ quáº£n trá»‹ viÃªn.",
            "intent": "ERROR",
            "query_results": []
        }
    
    # Step 2: Execute query
    query_results = await execute_query(
        analysis.get("query_type", "fallback"),
        analysis.get("entities", {})
    )
    
    # Step 3: Format response
    if query_results:
        response = await format_response(user_query, query_results, system_prompt)
    else:
        # Fallback
        model = genai.GenerativeModel(model_name=GEMINI_MODEL)
        try:
            # Short fallback prompt
            prompt = f"""
            User: {user_query}
            Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung vá» visa/du há»c Ãšc. 
            Báº®T BUá»˜C dÃ¹ng emoji cho cÃ¡c Ã½ chÃ­nh (ðŸŽ“, ðŸ›‚, ðŸ’°...). TrÃ¬nh bÃ y Ä‘áº¹p.
            """
            fallback_response = await model.generate_content_async(prompt)
            response = fallback_response.text
        except Exception as e:
            print(f"Fallback error: {e}")
            response = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p."
    
    return {
        "response": response,
        "intent": analysis.get("intent"),
        "query_results": query_results
    }


async def chatbot_response_stream(user_query: str, system_prompt: str) -> AsyncGenerator[str, None]:
    """
    Stream chatbot response chunk by chunk for real-time display
    
    Args:
        user_query: User's question
        system_prompt: System prompt for context
        
    Yields:
        Response chunks as they are generated
    """
    # Check cache first
    cache_key = f"stream:{user_query.lower().strip()}"
    cached = _get_cache(cache_key)
    if cached:
        # Stream cached response word by word for smooth UX
        words = cached.split()
        for i, word in enumerate(words):
            yield word + (" " if i < len(words) - 1 else "")
            await asyncio.sleep(0.01)  # Small delay for smooth streaming
        return
    
    # Step 1: Detect intent
    analysis = await detect_intent(user_query, system_prompt)
    
    if analysis.get("query_type") == "greeting":
        yield "ChÃ o báº¡n! TÃ´i lÃ  trá»£ lÃ½ áº£o AusVisa. TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n vá» du há»c vÃ  visa Ãšc?"
        return

    # Step 2: Execute query
    query_results = await execute_query(
        analysis.get("query_type", "fallback"),
        analysis.get("entities", {})
    )
    
    # Step 3: Stream format_response
    model = genai.GenerativeModel(model_name=GEMINI_MODEL)
    
    if query_results:
        prompt = f"""
        {system_prompt}
        User: "{user_query}"
        Data: {json.dumps(query_results[:5], ensure_ascii=False)}
        
        HÃ£y tráº£ lá»i tháº­t sinh Ä‘á»™ng vÃ  báº¯t máº¯t:
        1. Báº®T BUá»˜C dÃ¹ng biá»ƒu tÆ°á»£ng (emoji) cho Táº¤T Cáº¢ cÃ¡c tiÃªu Ä‘á» vÃ  Ã½ chÃ­nh.
        2. VÃ­ dá»¥: ðŸŽ“ Du há»c, ðŸ›‚ Visa, ðŸ’° Chi phÃ­, ðŸ“… Thá»i gian, âœ… Äiá»u kiá»‡n, ðŸ« TrÆ°á»ng há»c.
        3. TrÃ¬nh bÃ y dáº¡ng danh sÃ¡ch (bullet points) dá»… Ä‘á»c.
        """
        prompt = f"""
        User: "{user_query}"
        Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung vá» visa/du há»c Ãšc.
        Báº®T BUá»˜C dÃ¹ng emoji cho cÃ¡c Ã½ chÃ­nh (ðŸŽ“, ðŸ›‚, ðŸ’°...). TrÃ¬nh bÃ y Ä‘áº¹p.
        """
    
    try:
        # Stream response from Gemini using native async stream
        full_response = ""
        response_stream = await model.generate_content_async(prompt, stream=True)
        
        async for chunk in response_stream:
            if chunk.text:
                full_response += chunk.text
                yield chunk.text
        
        # Cache the complete response
        _set_cache(cache_key, full_response)
        
    except Exception as e:
        import traceback
        # Write to file instead of print for debugging
        # Write to file instead of print for debugging
        if "429" in str(e) or "quota" in str(e).lower():
             yield "âš ï¸ Há»‡ thá»‘ng Ä‘ang quÃ¡ táº£i (Google API Quota Exceeded). Vui lÃ²ng thá»­ láº¡i sau."
             return

        with open("streaming_error.log", "a", encoding="utf-8") as f:
            f.write(f"\n{'='*60}\n")
            f.write(f"Streaming error at: {datetime.now()}\n")
            f.write(f"Error: {e}\n")
            f.write(f"Traceback:\n{traceback.format_exc()}\n")
        
        error_msg = "Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i."
        yield error_msg
